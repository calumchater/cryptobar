services:
  questdb:
    image: questdb/questdb
    container_name: questdb
    restart: always
    ports:
      - 9000:9000
      - 9009:9009
      - 8812:8812
      - 9003:9003
    volumes:
      - ./questdb:/root/.questdb
    environment:
      - QDB_LOG_W_STDOUT_LEVEL=ERROR
      - QDB_LOG_W_FILE_LEVEL=ERROR
      - QDB_LOG_W_HTTP_MIN_LEVEL=ERROR
      - QDB_SHARED_WORKER_COUNT=1 
      - QDB_PG_USER="postgres" 
      - QDB_PG_PASSWORD="postgres" 
      - QDB_TELEMETRY_ENABLED=false 
      
  # Postgres DB for storing the news articles and the sentiments
  pg-db:
    image: postgres:latest
    volumes:
      - db:/var/lib/postrgresql/data/
    ports:
      - "5432:5432"
    environment:
      POSTGRES_PASSWORD: 'postgres'
      POSTGRES_DB: 'news_db'
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -d $${POSTGRES_DB} -U postgres"]
      interval: 10s
      timeout: 5s
      retries: 10
      start_period: 3s

  kafka-gen:
    image: confluentinc/cp-kafka:7.3.3
    hostname: kafka-gen
    container_name: kafka-gen
    volumes:
      - ./scripts/create_cluster_id.sh:/tmp/create_cluster_id.sh
      - ./clusterID:/tmp/clusterID
    command: "bash -c '/tmp/create_cluster_id.sh'"

  kafka:
    image: confluentinc/cp-kafka:7.3.3
    hostname: kafka
    container_name: kafka
    ports:
      - "39092:39092"
    environment:
      KAFKA_LISTENERS: BROKER://kafka1:19092,EXTERNAL://kafka1:39092,CONTROLLER://kafka1:9093
      KAFKA_ADVERTISED_LISTENERS: BROKER://kafka1:19092,EXTERNAL://kafka1:39092
      KAFKA_INTER_BROKER_LISTENER_NAME: BROKER
      KAFKA_CONTROLLER_LISTENER_NAMES: CONTROLLER
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: CONTROLLER:PLAINTEXT,BROKER:PLAINTEXT,EXTERNAL:PLAINTEXT
      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0
      KAFKA_PROCESS_ROLES: 'controller,broker'
      KAFKA_NODE_ID: 1
      KAFKA_CONTROLLER_QUORUM_VOTERS: '1@kafka1:9093' # ,2@kafka2:9093,3@kafka3:9093'
      KAFKA_METADATA_LOG_SEGMENT_MS: 15000
      KAFKA_METADATA_MAX_RETENTION_MS: 1200000
      KAFKA_METADATA_LOG_MAX_RECORD_BYTES_BETWEEN_SNAPSHOTS: 2800
      KAFKA_LOG_DIRS: '/tmp/kraft-combined-logs'
    volumes:
      - kafka-data:/var/lib/kafka/data
      - ./scripts/update_run.sh:/tmp/update_run.sh
      - ./clusterID:/tmp/clusterID
    command: "bash -c /etc/confluent/docker/run"  # -c '/tmp/update_run.sh' &&  

  # Gets price and order information 
  kraken:
    build:
      context: .
      args:
        INSTALL_DEPENDENCIES: dev
    depends_on:
      questdb:
        condition: service_healthy
      pg-db:
        condition: service_healthy
    ports:
      - "9000:9000"
    volumes:
      - .:/app:cached

  # News API to retrieve and summarise news articles
  news:
    build:
      context: .
      args:
        INSTALL_DEPENDENCIES: dev
    depends_on:
      pg-db:
        condition: service_healthy
    ports:
      - "8000:8000"
    volumes:
      - .:/app:cached

volumes:
  db: {}
  cache: {}
  kafka-data:
